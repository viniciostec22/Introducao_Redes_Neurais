{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODpQXuNFvIS+B2Jq5dnwDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciostec22/Introducao_Redes_Neurais/blob/main/atividade_01_redes_neurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nJK8MBib75i",
        "outputId": "cd9d0148-e918-4e68-aac6-5fee4ece8057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor\n",
        "import tqdm.notebook as tq\n",
        "import os\n"
      ],
      "metadata": {
        "id": "fIttfgOdcDrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/Mestrado_CNN/Dataset_banana/resized/resized'\n",
        "# DATA_DIR = '/content/drive/MyDrive/datasets/cifar10'\n",
        "CATEGORIES = ['BLACK_SIGATOKA', 'HEALTHY_LEAVES', 'PANAMA_DISEASE', 'YELLOW_SIGATOKA']\n",
        "def get_path(relpath):\n",
        "  return os.path.join(BASE_DIR, relpath)"
      ],
      "metadata": {
        "id": "1r6jjgVzdhDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Rodando na {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Df-VxW9kPJy",
        "outputId": "4f534726-ead9-4abb-d4b4-da50b87a6cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rodando na cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obeter valores de noramalização"
      ],
      "metadata": {
        "id": "bRPrx0pCnycI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import Subset\n",
        "\n",
        "# # Defina a transformação sem normalização\n",
        "# transformacao_sem_normalizacao = transforms.Compose([\n",
        "#     transforms.Resize((150, 150)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Crie um conjunto de dados sem normalização\n",
        "# conjunto_dados_sem_normalizacao = datasets.ImageFolder(root=BASE_DIR, transform=transformacao_sem_normalizacao)\n",
        "\n",
        "# # Obtenha um índice aleatório\n",
        "# indices_aleatorios = torch.randperm(len(conjunto_dados_sem_normalizacao))[:tamanho_amostra]\n",
        "\n",
        "# # Crie uma subclasse do conjunto de dados usando os índices aleatórios\n",
        "# amostra_aleatoria = Subset(conjunto_dados_sem_normalizacao, indices_aleatorios)\n",
        "\n",
        "# # Calcule as médias e desvios padrão usando a amostra\n",
        "# medias = torch.stack([imagem.mean((1, 2)) for imagem, _ in amostra_aleatoria]).mean(0)\n",
        "# desvios_padrao = torch.stack([imagem.std((1, 2)) for imagem, _ in amostra_aleatoria]).mean(0)\n",
        "\n",
        "# print(\"Médias:\", medias)\n",
        "# print(\"Desvios Padrão:\", desvios_padrao)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIzC9heVh1zv",
        "outputId": "63d9b49e-e18b-4da5-fe07-60eacf54e8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Médias: tensor([0.4308, 0.5051, 0.3152])\n",
            "Desvios Padrão: tensor([0.1718, 0.1682, 0.1632])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Transformações que serão aplicadas às imagens\n",
        "transformacoes = transforms.Compose(\n",
        "    [\n",
        "    transforms.Resize((150, 150)),  # Ajuste o tamanho conforme necessário\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.4308, 0.5051, 0.3152),\n",
        "        (0.1718, 0.1682, 0.1632),\n",
        "\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Crie um conjunto de dados personalizado\n",
        "dataset = datasets.ImageFolder(root=BASE_DIR, transform=transformacoes)\n",
        "\n",
        "# Divida o conjunto de dados em treino, validação e teste\n",
        "tamanho_treino = int(0.7 * len(dataset))  # 70% para treino\n",
        "tamanho_validacao = int(0.15 * len(dataset))  # 15% para validação\n",
        "tamanho_teste = len(dataset) - tamanho_treino - tamanho_validacao  # O restante para teste\n",
        "\n",
        "conjunto_treino, conjunto_validacao, conjunto_teste = random_split(dataset, [tamanho_treino, tamanho_validacao, tamanho_teste])\n"
      ],
      "metadata": {
        "id": "OAnkRpY8mSoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie DataLoader para cada conjunto\n",
        "batch_size = 64\n",
        "\n",
        "loader_treino = DataLoader(conjunto_treino, batch_size=batch_size, shuffle=True)\n",
        "loader_validacao = DataLoader(conjunto_validacao, batch_size=batch_size, shuffle=True)\n",
        "loader_teste = DataLoader(conjunto_teste, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "j2eefcMQh-dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualizar_tamanhos_dataset(loader, banana):\n",
        "    print(f\"Tamanhos do conjunto de dados {banana}:\")\n",
        "    print(f\"Número total de amostras: {len(loader.dataset)}\")\n",
        "    print(f\"Número de lotes por época: {len(loader)}\")\n",
        "    print(f\"Tamanho do lote: {loader.batch_size}\")\n",
        "\n",
        "# Exemplo de uso:\n",
        "visualizar_tamanhos_dataset(loader_treino, \"treino\")\n",
        "visualizar_tamanhos_dataset(loader_validacao, \"validação\")\n",
        "visualizar_tamanhos_dataset(loader_teste, \"teste\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRnUG5nElifv",
        "outputId": "5101946d-1260-4c8d-aeb6-406f0bf225c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanhos do conjunto de dados treino:\n",
            "Número total de amostras: 1748\n",
            "Número de lotes por época: 28\n",
            "Tamanho do lote: 64\n",
            "Tamanhos do conjunto de dados validação:\n",
            "Número total de amostras: 374\n",
            "Número de lotes por época: 6\n",
            "Tamanho do lote: 64\n",
            "Tamanhos do conjunto de dados teste:\n",
            "Número total de amostras: 376\n",
            "Número de lotes por época: 6\n",
            "Tamanho do lote: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=BASE_DIR, transform=transformacoes)\n"
      ],
      "metadata": {
        "id": "80BuSwo9dfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalModel(nn.Module):\n",
        "  def __init__(self, dropout_prob=0.5):\n",
        "      super().__init__()\n",
        "      self.convlayers = nn.Sequential(\n",
        "        nn.Conv2d(3, 256, kernel_size=(11, 11)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Dropout(dropout_prob),\n",
        "\n",
        "        nn.Conv2d(256, 128, kernel_size=(9, 9)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Dropout(dropout_prob),\n",
        "\n",
        "        nn.Conv2d(128, 64, kernel_size=(5, 5)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "\n",
        "        # nn.Conv2d(64, 32, kernel_size=(3, 3)),\n",
        "        # nn.ReLU(),\n",
        "        # nn.MaxPool2d(2, 2),\n",
        "\n",
        "      )\n",
        "      self._compute_linear_input_size()\n",
        "      self.linearlayers = nn.Sequential(\n",
        "            nn.Linear(self.linear_input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 4),\n",
        "            nn.Softmax(dim=1)  # Adicionando softmax\n",
        "        )\n",
        "\n",
        "  def _compute_linear_input_size(self):\n",
        "        # Crie uma entrada de exemplo e calcule o tamanho\n",
        "        # antes de criar as camadas lineares\n",
        "        x = torch.randn(1, 3, 150, 150)\n",
        "        x = self.convlayers(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        self.linear_input_size = x.size(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.convlayers(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.linearlayers(x)"
      ],
      "metadata": {
        "id": "OFEE9nj5i0Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convmodel = ConvolutionalModel().to(device)"
      ],
      "metadata": {
        "id": "BNUIdqg5kU2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_optimizer = torch.optim.SGD(convmodel.parameters(), lr=0.001)\n",
        "convlossfunc = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Ro9FUpPkk8QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 31"
      ],
      "metadata": {
        "id": "goQYzAVroGny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def treinar_modelo(modelo, loader_treino, loader_validacao, optimizer, loss_func, num_epochs=epochs):\n",
        "\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         modelo.train()\n",
        "#         for imagens, labels in loader_treino:\n",
        "#             imagens, labels = imagens.to(device), labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             saidas = modelo(imagens)\n",
        "#             perda = loss_func(saidas, labels)\n",
        "#             perda.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#         # Avaliação no conjunto de validação\n",
        "#         modelo.eval()\n",
        "#         acuracia_total = 0.0\n",
        "#         total_samples = 0\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for imagens, labels in loader_validacao:\n",
        "#                 imagens, labels = imagens.to(device), labels.to(device)\n",
        "\n",
        "#                 saidas = modelo(imagens)\n",
        "#                 _, previsto = torch.max(saidas, 1)\n",
        "\n",
        "#                 acuracia_total += torch.sum(previsto == labels).item()\n",
        "#                 total_samples += labels.size(0)\n",
        "\n",
        "#         acuracia = acuracia_total / total_samples\n",
        "#         print(f\"Época {epoch+1}/{num_epochs}, Perda: {perda.item():.4f}, Acurácia de Validação: {acuracia:.4f}\")\n",
        "\n",
        "#     print(\"Treinamento concluído.\")\n"
      ],
      "metadata": {
        "id": "V5VkQMFZn-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinar_modelo(convmodel, loader_treino, loader_validacao, conv_optimizer, convlossfunc, num_epochs=epochs)\n"
      ],
      "metadata": {
        "id": "5jyaELhlpJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def avaliar_modelo(modelo, loader, dispositivo):\n",
        "    modelo.eval()\n",
        "    acuracia_total = 0.0\n",
        "    total_samples = 0\n",
        "    perda_total = 0.0\n",
        "\n",
        "    criterio = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imagens, labels in loader:\n",
        "            imagens, labels = imagens.to(dispositivo), labels.to(dispositivo)\n",
        "\n",
        "            saidas = modelo(imagens)\n",
        "            perda_total += criterio(saidas, labels).item()\n",
        "\n",
        "            _, previsto = torch.max(saidas, 1)\n",
        "\n",
        "            acuracia_total += torch.sum(previsto == labels).item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    acuracia = acuracia_total / total_samples\n",
        "    perda_media = perda_total / len(loader)\n",
        "\n",
        "    return acuracia, perda_media\n",
        "\n",
        "def treinar_avaliar_modelo(modelo, loader_treino, loader_validacao, loader_teste, optimizer, loss_func, num_epochs=epochs):\n",
        "\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo = modelo.to(dispositivo)\n",
        "\n",
        "    criterio = nn.CrossEntropyLoss()\n",
        "    otimizador = optimizer\n",
        "\n",
        "   # ...\n",
        "    perda_treino_lista = []\n",
        "    perda_validacao_lista = []\n",
        "    perda_teste_lista = []\n",
        "    acuracia_validacao_lista = []\n",
        "    acuracia_teste_lista = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        modelo.train()\n",
        "        progress_bar = tqdm(loader_treino, desc=f\"Época {epoch+1}/{num_epochs}\", leave=False)\n",
        "        perda_total_epoch = 0.0  # Inicializar a perda total da época\n",
        "\n",
        "        for imagens, labels in progress_bar:\n",
        "            imagens, labels = imagens.to(dispositivo), labels.to(dispositivo)\n",
        "\n",
        "            otimizador.zero_grad()\n",
        "            saidas = modelo(imagens)\n",
        "            perda = criterio(saidas, labels)\n",
        "            perda.backward()\n",
        "            otimizador.step()\n",
        "\n",
        "            progress_bar.set_postfix({\"Perda Treino\": perda.item()})\n",
        "\n",
        "            perda_total_epoch += perda.item()  # Acumular perda durante a época\n",
        "\n",
        "        perda_media = perda_total_epoch / len(loader_treino)  # Calcular a média da perda\n",
        "        perda_treino_lista.append(perda_media)\n",
        "\n",
        "        # Avaliação no conjunto de validação\n",
        "        acuracia_validacao, perda_validacao = avaliar_modelo(modelo, loader_validacao, dispositivo)\n",
        "        perda_validacao_lista.append(perda_validacao)\n",
        "        acuracia_validacao_lista.append(acuracia_validacao)\n",
        "\n",
        "        # Avaliação no conjunto de teste\n",
        "        acuracia_teste, perda_teste = avaliar_modelo(modelo, loader_teste, dispositivo)\n",
        "        perda_teste_lista.append(perda_teste)\n",
        "        acuracia_teste_lista.append(acuracia_teste)\n",
        "\n",
        "        print(f\"Época {epoch+1}/{num_epochs}, Perda Treino: {perda_media:.4f}, Perda Validação: {perda_validacao:.4f}, Perda Teste: {perda_teste:.4f}, Acurácia Validação: {acuracia_validacao:.4f}, Acurácia Teste: {acuracia_teste:.4f}\")\n",
        "  # ...\n",
        "\n",
        "\n",
        "\n",
        "    return perda_treino_lista, perda_validacao_lista, perda_teste_lista, acuracia_validacao_lista, acuracia_teste_lista\n",
        "\n",
        "# Exemplo de uso:\n",
        "perda_treino, perda_validacao, perda_teste, acuracia_validacao, acuracia_teste = treinar_avaliar_modelo(convmodel, loader_treino, loader_validacao, loader_teste, conv_optimizer, convlossfunc, num_epochs=epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "DX7jhTqvzJs3",
        "outputId": "dce12e0c-74fc-4f47-b097-e99fa4277fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-88346ba9bc99>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Exemplo de uso:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mperda_treino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperda_validacao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperda_teste\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macuracia_validacao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macuracia_teste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreinar_avaliar_modelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_treino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_validacao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_teste\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvlossfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-88346ba9bc99>\u001b[0m in \u001b[0;36mtreinar_avaliar_modelo\u001b[0;34m(modelo, loader_treino, loader_validacao, loader_teste, optimizer, loss_func, num_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mperda_total_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m  \u001b[0;31m# Inicializar a perda total da época\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimagens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mimagens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimagens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispositivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispositivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_num_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(perda_treino))\n",
        "print(len(perda_validacao))\n",
        "print(len(perda_teste))\n",
        "print(len(acuracia_validacao))\n",
        "print(len(acuracia_teste))\n"
      ],
      "metadata": {
        "id": "h-zKLHWX0AzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "num_epochs = epochs\n",
        "def plotar_graficos(perda_treino, perda_validacao, perda_teste, acuracia_validacao, acuracia_teste, num_epochs):\n",
        "    # Gráfico de perda\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), perda_treino, label='Treino')\n",
        "    plt.plot(range(1, num_epochs + 1), perda_validacao, label='Validação')\n",
        "    plt.plot(range(1, num_epochs + 1), perda_teste, label='Teste')\n",
        "    plt.title('Perda ao Longo das Épocas')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Gráfico de acurácia\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), acuracia_validacao, label='Validação')\n",
        "    plt.plot(range(1, num_epochs + 1), acuracia_teste, label='Teste')\n",
        "    plt.title('Acurácia ao Longo das Épocas')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "def plotar_matriz_confusao(modelo, loader, dispositivo):\n",
        "    modelo.eval()\n",
        "    predicoes = []\n",
        "    alvos = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imagens, labels in loader:\n",
        "            imagens, labels = imagens.to(dispositivo), labels.to(dispositivo)\n",
        "\n",
        "            saidas = modelo(imagens)\n",
        "            _, previsto = torch.max(saidas, 1)\n",
        "\n",
        "            predicoes.extend(previsto.cpu().numpy())\n",
        "            alvos.extend(labels.cpu().numpy())\n",
        "\n",
        "    matriz_confusao = confusion_matrix(alvos, predicoes)\n",
        "    classes = [str(i) for i in range(matriz_confusao.shape[0])]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(matriz_confusao, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Matriz de Confusão')\n",
        "    plt.xlabel('Previsto')\n",
        "    plt.ylabel('Real')\n",
        "    plt.show()\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "# Exemplo de uso:\n",
        "plotar_graficos(perda_treino, perda_validacao, perda_teste, acuracia_validacao, acuracia_teste, num_epochs)\n",
        "plotar_matriz_confusao(convmodel, loader_teste, device)\n"
      ],
      "metadata": {
        "id": "oWO0X-wqy1yT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}